import streamlit as st
import tempfile
import os
import sys
import numpy as np
import time

# OpenCV/MediaPipeç’°å¢ƒè¨­å®šï¼ˆOpenGLç„¡åŠ¹åŒ–ï¼‰
os.environ['OPENCV_IO_ENABLE_OPENEXR'] = '0'
os.environ['OPENCV_IO_ENABLE_JASPER'] = '0'
os.environ['OPENCV_VIDEOIO_PRIORITY_MSMF'] = '0'
# Streamlit Cloudç’°å¢ƒå¤‰æ•°è¨­å®š
os.environ['MPLCONFIGDIR'] = '/tmp'

# OpenCVå®‰å…¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆStreamlit Cloudå¯¾å¿œï¼‰
try:
    import cv2
    # OpenCVè¨­å®šï¼ˆãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ç’°å¢ƒç”¨ï¼‰
    cv2.setUseOptimized(True)
    CV2_AVAILABLE = True
    st.success("âœ… OpenCV ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
except ImportError as e:
    st.error(f"OpenCVã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    st.info("requirements.txtã«opencv-python-headlessãŒå«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    CV2_AVAILABLE = False

# MediaPipe ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚¯ãƒ©ã‚¦ãƒ‰ç’°å¢ƒå¯¾å¿œï¼‰
try:
    import mediapipe as mp
    mp_pose = mp.solutions.pose
    mp_hands = mp.solutions.hands
    mp_drawing = mp.solutions.drawing_utils
    mp_styles = mp.solutions.drawing_styles
    MEDIAPIPE_AVAILABLE = True
    st.success("âœ… MediaPipe ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
except ImportError as e:
    st.error(f"MediaPipeã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    st.info("requirements.txtã«mediapipeãŒå«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    MEDIAPIPE_AVAILABLE = False
except Exception as e:
    st.error(f"MediaPipeåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    MEDIAPIPE_AVAILABLE = False

# ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
with st.expander("ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±", expanded=False):
    st.write(f"Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {sys.version}")
    st.write(f"OpenCV åˆ©ç”¨å¯èƒ½: {CV2_AVAILABLE}")
    st.write(f"MediaPipe åˆ©ç”¨å¯èƒ½: {MEDIAPIPE_AVAILABLE}")
    if CV2_AVAILABLE:
        st.write(f"OpenCV ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {cv2.__version__}")
    if MEDIAPIPE_AVAILABLE:
        st.write(f"MediaPipe ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {mp.__version__}")
    st.write(f"ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {os.getcwd()}")
    st.write(f"ç’°å¢ƒå¤‰æ•° DISPLAY: {os.environ.get('DISPLAY', 'Not set')}")

# Streamlitè¨­å®š
st.set_page_config(
    page_title="å§¿å‹¢æ¨å®šã‚¢ãƒ—ãƒª - MediaPipe Ã— Streamlit",
    page_icon="ğŸ§",
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items={
        'Get Help': 'https://github.com/tbw0619/pose-estimation-app',
        'Report a bug': 'https://github.com/tbw0619/pose-estimation-app/issues',
        'About': """
        # ğŸ§ å§¿å‹¢æ¨å®šã‚¢ãƒ—ãƒª
        
        YOLO7ã‚¹ã‚¿ã‚¤ãƒ«ã®é«˜ç²¾åº¦å§¿å‹¢æ¨å®šã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
        
        **é–‹ç™ºè€…**: tbw0619  
        **æŠ€è¡“**: MediaPipe Ã— Streamlit  
        **GitHub**: https://github.com/tbw0619/pose-estimation-app
        """
    }
)

# ã‚¿ã‚¤ãƒˆãƒ«
st.title("ğŸ§ å§¿å‹¢æ¨å®šã‚¢ãƒ—ãƒª - MediaPipe Ã— Streamlit")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§è¨­å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³
st.sidebar.header("âš™ï¸ è¨­å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³")

# è§£åƒåº¦è¨­å®š
resolution_option = st.sidebar.selectbox(
    "å‡¦ç†è§£åƒåº¦",
    ["å…ƒã®è§£åƒåº¦ã‚’ä¿æŒ", "HD (1280x720)", "Full HD (1920x1080)", "ä½è§£åƒåº¦ (640x480)"],
    index=0
)

# ç²¾åº¦è¨­å®š
model_complexity = st.sidebar.slider("ãƒ¢ãƒ‡ãƒ«è¤‡é›‘åº¦ (é«˜ã„ã»ã©æ­£ç¢º)", 0, 2, 2)
detection_confidence = st.sidebar.slider("æ¤œå‡ºä¿¡é ¼åº¦", 0.0, 1.0, 0.7, 0.05)
tracking_confidence = st.sidebar.slider("è¿½è·¡ä¿¡é ¼åº¦", 0.0, 1.0, 0.5, 0.05)

# æç”»è¨­å®š
draw_landmarks = st.sidebar.checkbox("é–¢ç¯€ç‚¹ã‚’æç”»", True)
draw_connections = st.sidebar.checkbox("éª¨æ ¼ç·šã‚’æç”»", True)
draw_face = st.sidebar.checkbox("é¦–ã‚’æç”»", True, help="è‚©é–¢ç¯€ã¨ã“ã‚ã‹ã¿ã‚’çµã‚“ã§é¦–ã‚’è¡¨ç¾")
draw_hands = st.sidebar.checkbox("æ‰‹ã‚’æç”»", True)
landmark_size = st.sidebar.slider("é–¢ç¯€ç‚¹ã‚µã‚¤ã‚º", 1, 10, 3)
connection_thickness = st.sidebar.slider("éª¨æ ¼ç·šã®å¤ªã•", 1, 10, 2)

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®å¯ç”¨æ€§ãƒã‚§ãƒƒã‚¯
if not CV2_AVAILABLE:
    st.error("âŒ OpenCVãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ã‚¢ãƒ—ãƒªã‚’æ­£å¸¸ã«å‹•ä½œã•ã›ã‚‹ãŸã‚ã«ã¯OpenCVãŒå¿…è¦ã§ã™ã€‚")
    st.stop()

if not MEDIAPIPE_AVAILABLE:
    st.error("âŒ MediaPipeãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ã‚¢ãƒ—ãƒªã‚’æ­£å¸¸ã«å‹•ä½œã•ã›ã‚‹ãŸã‚ã«ã¯MediaPipeãŒå¿…è¦ã§ã™ã€‚")
    st.stop()

# MediaPipeåˆæœŸåŒ–ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆä½¿ç”¨ï¼‰
@st.cache_resource
def initialize_mediapipe():
    """MediaPipeãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ï¼ˆè»½é‡ç‰ˆï¼‰"""
    try:
        mp_drawing = mp.solutions.drawing_utils
        mp_face_mesh = mp.solutions.face_mesh
        
        # è»½é‡åˆæœŸåŒ–ã®ã¿ï¼ˆäº‹å‰ãƒ†ã‚¹ãƒˆã¯ç„¡åŠ¹åŒ–ï¼‰
        st.info("MediaPipeè»½é‡åˆæœŸåŒ–ä¸­...")
        return mp_drawing, mp_face_mesh, True
        
    except Exception as e:
        st.error(f"MediaPipeåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None, None, False

try:
    mp_drawing, mp_face_mesh, mediapipe_available = initialize_mediapipe()
    if mediapipe_available:
        st.success("âœ… MediaPipe ãŒæ­£å¸¸ã«èª­ã¿è¾¼ã¾ã‚Œã¾ã—ãŸï¼ˆYOLO7ã‚¹ã‚¿ã‚¤ãƒ«å§¿å‹¢æ¨å®šï¼‰")
    else:
        st.error("âŒ MediaPipe ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        st.stop()
except Exception as e:
    st.error(f"âŒ MediaPipe ã®åˆæœŸåŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    st.stop()

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è¨­å®šã®èª¬æ˜
st.info("ğŸ“ å¯¾å¿œãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: MP4, MOV, AVIï¼ˆã‚µã‚¤ã‚ºåˆ¶é™ãªã— - å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã¯å‡¦ç†ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰")

# å‹•ç”»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ€ãƒ¼
uploaded_file = st.file_uploader(
    "å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", 
    type=["mp4", "mov", "avi"],
    help="å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚‚å‡¦ç†ã§ãã¾ã™ãŒã€æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚"
)

def get_target_resolution(original_width, original_height, resolution_option):
    """è§£åƒåº¦è¨­å®šã«åŸºã¥ã„ã¦ç›®æ¨™è§£åƒåº¦ã‚’è¨ˆç®—"""
    if resolution_option == "å…ƒã®è§£åƒåº¦ã‚’ä¿æŒ":
        return original_width, original_height
    elif resolution_option == "HD (1280x720)":
        target_width, target_height = 1280, 720
    elif resolution_option == "Full HD (1920x1080)":
        target_width, target_height = 1920, 1080
    elif resolution_option == "ä½è§£åƒåº¦ (640x480)":
        target_width, target_height = 640, 480
    else:
        return original_width, original_height
    
    # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’ç¶­æŒã—ã¦ãƒªã‚µã‚¤ã‚º
    aspect_ratio = original_width / original_height
    target_aspect_ratio = target_width / target_height
    
    if aspect_ratio > target_aspect_ratio:
        # å¹…ã«åˆã‚ã›ã¦ãƒªã‚µã‚¤ã‚º
        new_width = target_width
        new_height = int(target_width / aspect_ratio)
    else:
        # é«˜ã•ã«åˆã‚ã›ã¦ãƒªã‚µã‚¤ã‚º
        new_height = target_height
        new_width = int(target_height * aspect_ratio)
    
    return new_width, new_height

def draw_pose_landmarks(frame, pose_results, face_results, hands_results, mp_pose, mp_face_mesh, mp_hands, mp_drawing, 
                       draw_landmarks, draw_connections, draw_face, draw_hands, landmark_size, connection_thickness):
    """å§¿å‹¢ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ã‚’æç”»ï¼ˆYOLO7ã‚¹ã‚¿ã‚¤ãƒ«ã®é¦–æç”»ï¼‰"""
    
    # å§¿å‹¢ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯æç”»
    if pose_results.pose_landmarks:
        # ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯æç”»è¨­å®š
        landmark_drawing_spec = mp_drawing.DrawingSpec(
            color=(0, 255, 0),  # ç·‘è‰²ã®é–¢ç¯€ç‚¹
            thickness=landmark_size,
            circle_radius=landmark_size
        )
        
        connection_drawing_spec = mp_drawing.DrawingSpec(
            color=(255, 0, 0),  # èµ¤è‰²ã®éª¨æ ¼ç·š
            thickness=connection_thickness
        )
        
        # æç”»å®Ÿè¡Œ
        if draw_landmarks and draw_connections:
            mp_drawing.draw_landmarks(
                frame, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS,
                landmark_drawing_spec=landmark_drawing_spec,
                connection_drawing_spec=connection_drawing_spec
            )
        elif draw_landmarks:
            # é–¢ç¯€ç‚¹ã®ã¿æç”»
            mp_drawing.draw_landmarks(
                frame, pose_results.pose_landmarks, None,
                landmark_drawing_spec=landmark_drawing_spec
            )
        elif draw_connections:
            # éª¨æ ¼ç·šã®ã¿æç”»
            mp_drawing.draw_landmarks(
                frame, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS,
                landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0,0,0), thickness=0, circle_radius=0),
                connection_drawing_spec=connection_drawing_spec
            )
        
        # YOLO7ã‚¹ã‚¿ã‚¤ãƒ«ã®é¦–æç”»
        if draw_face:
            # è‚©ã®åº§æ¨™ã‚’å–å¾—
            left_shoulder = pose_results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER]
            right_shoulder = pose_results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER]
            
            # é¼»ã®åº§æ¨™ã‚’å–å¾—ï¼ˆã“ã‚ã‹ã¿ã®ä»£ã‚ã‚Šã«ä½¿ç”¨ã€ã‚ˆã‚Šå®‰å®šï¼‰
            nose = pose_results.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE]
            
            # å·¦å³ã®è€³ã®åº§æ¨™ã‚‚å–å¾—ï¼ˆã“ã‚ã‹ã¿ã«ã‚ˆã‚Šè¿‘ã„ä½ç½®ï¼‰
            left_ear = pose_results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_EAR]
            right_ear = pose_results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_EAR]
            
            h, w, _ = frame.shape
            
            # é¦–ã®ä¸­å¿ƒè¨ˆç®—ï¼ˆè‚©ã®ä¸­ç‚¹ï¼‰
            neck_center_x = (left_shoulder.x + right_shoulder.x) / 2
            neck_center_y = (left_shoulder.y + right_shoulder.y) / 2
            
            # é ­éƒ¨ã®ä¸­å¿ƒè¨ˆç®—ï¼ˆé¼»ã¨è€³ã®å¹³å‡ï¼‰
            head_center_x = (nose.x + left_ear.x + right_ear.x) / 3
            head_center_y = (nose.y + left_ear.y + right_ear.y) / 3
            
            # YOLO7ã‚¹ã‚¿ã‚¤ãƒ«ï¼šè‚©ã‹ã‚‰é ­éƒ¨ã¸ã®é¦–ç·š
            cv2.line(frame,
                    (int(neck_center_x * w), int(neck_center_y * h)),
                    (int(head_center_x * w), int(head_center_y * h)),
                    (0, 255, 255), connection_thickness * 2)  # é»„è‰²ã®é¦–ç·š
            
            # å·¦è‚©ã‹ã‚‰å·¦ã“ã‚ã‹ã¿ï¼ˆå·¦è€³ï¼‰
            cv2.line(frame,
                    (int(left_shoulder.x * w), int(left_shoulder.y * h)),
                    (int(left_ear.x * w), int(left_ear.y * h)),
                    (0, 200, 255), connection_thickness)  # ã‚ªãƒ¬ãƒ³ã‚¸è‰²
            
            # å³è‚©ã‹ã‚‰å³ã“ã‚ã‹ã¿ï¼ˆå³è€³ï¼‰
            cv2.line(frame,
                    (int(right_shoulder.x * w), int(right_shoulder.y * h)),
                    (int(right_ear.x * w), int(right_ear.y * h)),
                    (0, 200, 255), connection_thickness)  # ã‚ªãƒ¬ãƒ³ã‚¸è‰²
            
            # é¦–ã®é–¢ç¯€ç‚¹ã‚’æç”»
            cv2.circle(frame,
                      (int(neck_center_x * w), int(neck_center_y * h)),
                      landmark_size * 2, (0, 255, 255), -1)  # é»„è‰²ã®é¦–é–¢ç¯€
            
            # é ­éƒ¨ä¸­å¿ƒç‚¹ã‚’æç”»
            cv2.circle(frame,
                      (int(head_center_x * w), int(head_center_y * h)),
                      landmark_size, (255, 255, 0), -1)  # é’ç·‘è‰²ã®é ­éƒ¨ä¸­å¿ƒ
    
    # é¡”ã®è¼ªéƒ­æç”»ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€é¡”ãƒ¡ãƒƒã‚·ãƒ¥ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã®ã¿ï¼‰
    if draw_face and face_results and face_results.multi_face_landmarks:
        for face_landmarks in face_results.multi_face_landmarks:
            # é¡”ã®ä¸»è¦ãªè¼ªéƒ­ã®ã¿æç”»ï¼ˆå‡¦ç†ã‚’è»½ãã™ã‚‹ï¼‰
            mp_drawing.draw_landmarks(
                frame, face_landmarks, mp_face_mesh.FACEMESH_CONTOURS,
                landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255, 255, 0), thickness=1, circle_radius=0),
                connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 255, 0), thickness=1)
            )
    
    # æ‰‹ã®æç”»
    if draw_hands and hands_results.multi_hand_landmarks:
        for hand_landmarks in hands_results.multi_hand_landmarks:
            mp_drawing.draw_landmarks(
                frame, hand_landmarks, mp_hands.HAND_CONNECTIONS,
                landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 255), thickness=landmark_size, circle_radius=landmark_size),
                connection_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 255), thickness=connection_thickness)
            )
    
    return frame

if uploaded_file is not None and mediapipe_available:
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºè¡¨ç¤ºï¼ˆåˆ¶é™ãªã—ï¼‰
        file_size = len(uploaded_file.getvalue())
        st.write(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size / (1024*1024):.1f} MB")
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼è¡¨ç¤º
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        status_text.text("ğŸ”„ å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­...")
        progress_bar.progress(5)
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tfile:
            tfile.write(uploaded_file.getvalue())
            video_path = tfile.name
        
        progress_bar.progress(10)
        status_text.text("ğŸ“¹ å‹•ç”»æƒ…å ±ã‚’å–å¾—ä¸­...")
        
        # å‹•ç”»èª­ã¿è¾¼ã¿
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            st.error("âŒ å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã‘ã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            os.unlink(video_path)
            st.stop()
        
        # å‹•ç”»æƒ…å ±å–å¾—
        original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        duration = total_frames / fps if fps > 0 else 0
        
        # ç›®æ¨™è§£åƒåº¦è¨ˆç®—
        target_width, target_height = get_target_resolution(original_width, original_height, resolution_option)
        
        # å‹•ç”»æƒ…å ±è¡¨ç¤º
        col1, col2 = st.columns(2)
        with col1:
            st.write("ğŸ“Š **å…ƒã®å‹•ç”»æƒ…å ±:**")
            st.write(f"- è§£åƒåº¦: {original_width} Ã— {original_height}")
            st.write(f"- ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {total_frames}")
            st.write(f"- FPS: {fps:.1f}")
            st.write(f"- æ™‚é–“: {duration:.1f} ç§’")
        
        with col2:
            st.write("âš™ï¸ **å‡¦ç†è¨­å®š:**")
            st.write(f"- å‡¦ç†è§£åƒåº¦: {target_width} Ã— {target_height}")
            st.write(f"- ãƒ¢ãƒ‡ãƒ«è¤‡é›‘åº¦: {model_complexity}")
            st.write(f"- æ¤œå‡ºä¿¡é ¼åº¦: {detection_confidence}")
            st.write(f"- è¿½è·¡ä¿¡é ¼åº¦: {tracking_confidence}")
        
        progress_bar.progress(20)
        status_text.text("ğŸ¤– MediaPipeåˆæœŸåŒ–ä¸­...")
        
        # å‹•ç”»è¡¨ç¤ºã‚¨ãƒªã‚¢
        video_placeholder = st.empty()
        
        # MediaPipeè¨­å®šï¼ˆã‚¯ãƒ©ã‚¦ãƒ‰ç’°å¢ƒå¯¾å¿œã§è»½é‡åŒ–ï¼‰
        # è»½é‡ãƒ¢ãƒ‡ãƒ«ã‚’å¼·åˆ¶ä½¿ç”¨ï¼ˆã‚¯ãƒ©ã‚¦ãƒ‰ç’°å¢ƒã§ã®æ¨©é™å•é¡Œå›é¿ï¼‰
        pose_config = {
            'static_image_mode': False,
            'model_complexity': 0,  # è»½é‡ãƒ¢ãƒ‡ãƒ«å¼·åˆ¶ä½¿ç”¨
            'smooth_landmarks': True,
            'enable_segmentation': False,
            'smooth_segmentation': False,
            'min_detection_confidence': max(0.5, detection_confidence),  # æœ€ä½0.5
            'min_tracking_confidence': max(0.5, tracking_confidence)  # æœ€ä½0.5
        }
        
        hands_config = {
            'static_image_mode': False,
            'max_num_hands': 2,
            'model_complexity': 0,  # è»½é‡ãƒ¢ãƒ‡ãƒ«å¼·åˆ¶ä½¿ç”¨
            'min_detection_confidence': max(0.5, detection_confidence),
            'min_tracking_confidence': max(0.5, tracking_confidence)
        }
        
        # MediaPipeå‡¦ç†ã‚’try-catchã§ãƒ©ãƒƒãƒ—
        try:
            with mp_pose.Pose(**pose_config) as pose, \
            mp_hands.Hands(**hands_config) as hands:
                
                progress_bar.progress(30)
                status_text.text("ğŸƒ å§¿å‹¢æ¨å®šå‡¦ç†ä¸­...")
                
                frame_count = 0
                processing_times = []
                
                # ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã®å‡¦ç†ãƒ«ãƒ¼ãƒ—
                while True:
                    ret, frame = cap.read()
                    if not ret:
                        break
                    
                    start_time = time.time()
                    
                    # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æŒ‡å®šè§£åƒåº¦ã«ãƒªã‚µã‚¤ã‚º
                    frame_resized = cv2.resize(frame, (target_width, target_height))
                    
                    # BGRã‹ã‚‰RGBã«å¤‰æ›
                    rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)
                    
                    # å§¿å‹¢æ¨å®š
                    pose_results = pose.process(rgb)
                    hands_results = hands.process(rgb)
                    
                    # æç”»
                    annotated_frame = draw_pose_landmarks(
                        frame_resized, pose_results, None, hands_results, 
                        mp_pose, mp_face_mesh, mp_hands, mp_drawing, 
                        draw_landmarks, draw_connections, draw_face, draw_hands,
                        landmark_size, connection_thickness
                    )
                    
                    # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚«ã‚¦ãƒ³ãƒˆã¨é€²æ—æ›´æ–°
                    frame_count += 1
                    progress = min(30 + (frame_count / total_frames) * 60, 90)
                    progress_bar.progress(int(progress))
                    
                    # å‹•ç”»è¡¨ç¤ºæ›´æ–°ï¼ˆä¸€å®šé–“éš”ã§ï¼‰
                    if frame_count % max(1, total_frames // 50) == 0:  # æœ€å¤§50å›æ›´æ–°
                        video_placeholder.image(annotated_frame, channels="RGB", use_column_width=True)
                    
                    # å‡¦ç†æ™‚é–“è¨ˆæ¸¬
                    processing_time = time.time() - start_time
                    processing_times.append(processing_time)
                    
                    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°ï¼ˆ100ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ï¼‰
                    if frame_count % 100 == 0:
                        avg_time = np.mean(processing_times[-100:]) * 1000
                        status_text.text(f"ğŸƒ å‡¦ç†ä¸­... {frame_count}/{total_frames} frames ({avg_time:.1f}ms/frame)")
                
                # æœ€çµ‚ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¡¨ç¤º
                if 'annotated_frame' in locals():
                    video_placeholder.image(annotated_frame, channels="RGB", use_column_width=True)
                
                cap.release()
                progress_bar.progress(100)
                
                # çµ±è¨ˆè¨ˆç®—
                avg_processing_time = np.mean(processing_times) * 1000
                status_text.text(f"âœ… å‡¦ç†å®Œäº†ï¼å¹³å‡å‡¦ç†æ™‚é–“: {avg_processing_time:.1f}ms/frame")
                
                # çµ±è¨ˆæƒ…å ±è¡¨ç¤º
                st.success("ğŸ‰ å§¿å‹¢æ¨å®šãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("å‡¦ç†ãƒ•ãƒ¬ãƒ¼ãƒ æ•°", f"{frame_count}")
                with col2:
                    st.metric("å¹³å‡å‡¦ç†æ™‚é–“", f"{avg_processing_time:.1f}ms")
                with col3:
                    estimated_fps = 1000 / avg_processing_time if avg_processing_time > 0 else 0
                    st.metric("æ¨å®šãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½", f"{estimated_fps:.1f} FPS")
                
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                try:
                    os.unlink(video_path)
                except:
                    pass
                    
        except Exception as e:
            st.error(f"âŒ å‹•ç”»å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            st.error(f"ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}")
            import traceback
            st.text("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:")
            st.code(traceback.format_exc())
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            try:
                cap.release()
                os.unlink(video_path)
            except:
                pass
                
    except Exception as e:
        st.error(f"âŒ å…¨ä½“çš„ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        st.code(traceback.format_exc())
        
elif uploaded_file is not None and not mediapipe_available:
    st.warning("âš ï¸ MediaPipeãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œã§ãã¾ã›ã‚“ã€‚")
else:
    st.info("ğŸ‘† ä¸Šè¨˜ã‹ã‚‰å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
